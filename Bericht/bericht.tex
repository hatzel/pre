\documentclass[
	12pt,
	a4paper,
	BCOR10mm,
	%chapterprefix,
	DIV14,
	listof=totoc,
	bibliography=totoc,
	headsepline
]{scrreprt}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{lmodern}

\usepackage[footnote]{acronym}
\usepackage[page,toc]{appendix}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{graphicx}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[htt]{hyphenat}
\usepackage{listings}
\usepackage{lscape}
\usepackage{microtype}
\usepackage{nicefrac}
\usepackage{subfig}
\usepackage{textcomp}
\usepackage[subfigure,titles]{tocloft}
\usepackage{units}
\usepackage{pgf}

\lstset{
	basicstyle=\ttfamily,
	frame=single,
	numbers=left,
	language=C,
	breaklines=true,
	breakatwhitespace=true,
	postbreak=\hbox{$\hookrightarrow$ },
	showstringspaces=false,
	tabsize=4
}

\renewcommand*{\lstlistlistingname}{Listing catalog}

\renewcommand*{\appendixname}{Appendix}
\renewcommand*{\appendixtocname}{Appendices}
\renewcommand*{\appendixpagename}{Appendices}

\newcommand*{\mcite}[1]{\footnote{\cite{#1}}}


\begin{document}

\begin{titlepage}
	\begin{center}
		{\titlefont\huge Titel der Arbeit\par}

		\bigskip
		\bigskip

		{\titlefont\Large --- Art der Arbeit ---\par}

		\bigskip
		\bigskip

		{\large Arbeitsbereich Wissenschaftliches Rechnen\\
		Fachbereich Informatik\\
		Fakultät für Mathematik, Informatik und Naturwissenschaften\\
		Universität Hamburg\par}
	\end{center}

	\vfill

	{\large \begin{tabular}{ll}
		Vorgelegt von: & Lukas Stabe \\
			& Hans Ole Hatzel \\
			& Arne Struck \\
		E-Mail-Adresse: 
			& \href{mailto:2stabe@informatik.uni-hamburg.de} {2stabe@informatik.uni-hamburg.de} \\			
			& \href{mailto:2hatzel@informatik.uni-hamburg.de}{2hatzel@informatik.uni-hamburg.de} \\
			& \href{mailto:1struck@informatik.uni-hamburg.de}{1struck@informatik.uni-hamburg.de} \\ 
		%Matrikelnummer: & 1234567 \\
		Studiengang: & Informatik \\
		\\
		Betreuer: & Michael Kuhn \\
			& Konstantinos Chasapis \\
		\\
		Hamburg, den 29.03.2015
	\end{tabular}\par}
\end{titlepage}

\chapter*{Abstract}
%TODO write

\thispagestyle{empty}

   

\tableofcontents

\chapter{Introduction}
\label{Introduction}
%TODO ausbaubar
Since and even before the start of the days of modern computers compression is of a certain importance. 
One example for compression before modern computers are commercial codes for telegraphing.
\\
The use case in these days was to minimize the package size for network communication.
Many possible utilizations for compression have been discovered since then.
Among others fast, light compression for running systems and slower compression with higher compression ratios for long term storing. 
\\
Especially but not only in High Performance Computing (HPC) we see an emerging gap between the possibilities to create data and the ability to store it in an appropriate way. 
The gap is based on the reducing difference between computing speed and storage capacity\mcite{ExaStoSy}.
This effect can be observed when looking at new generations of supercomputers. 
In the case of the dkrz the storage capacity may only increase by a factor of three while computing performance is expected to grow by a factor of twenty\mcite{ExaStoSy}.
Compression can not solve this problem, but it can reduce the impact of the gap. 
\\
However, compression is a superordinate concept.
There are many possible ways (algorithms) to achieve compression.
The arising question is for which use case which way is the adequate one.
There is an uncountable number of different ways to compress.
This means the optimal solution to this problem is not detectable.
But it is possible to get an approximation through testing as many algortihms as possible an comparing the results.
That is the main point of this analysis of compression algorithms in the HPC field.


\chapter{Assignment and Methodology}
\label{Assignment and Methodology}
%TODO ausbauen
\section*{Assignment}
The main assignment for this project is the performance analysis for different compression algorithms in the HPC field. 
\\
Therefore we have to analyse them for the different use cases of compression in the HPC field. 
The main use cases are the storage in the life system and the storage in the archive system.
\\
Furthermore we have to look at the decompression speed and the ways it effects our two cases. 
\\
Analysing requires tools to get information about data. There are existing tools for testing multiple compression algorithms on a single file. So one task is to find a way to test multiple files with multiple algorithms.


\section*{Methodology}
We approached this through writing a script around a tool for testing multiple compression algorithms so we could use it for multiple files and store the results in a database. 
\\
Multiple files were necessary to get an appropriate amount of measurements.
For this purpose different datasets from scientific sources were required.
\\
After selecting and getting suitable data sets and testing algorithms on them with our script we needed to analyse the different outputs.
To do so we created a metric which can be weighted, so it is possible to map the results to our use cases in an appropriate way. \\

\chapter{Design and Implementation}
\label{Design}
%TODO expand
We implemented a script to test compression algorithms on multiple datasets based on the open source tool fsbench. 
Fsbench is written in C++ and has the option to output the data it gathers, machine readable, as csv.
However the tool is not capable of batch processing multiple files. 
This is where our script comes in, we intend to provide a simple interface to test entire directory structures and store them in easy to use formats.
We decided to go with sqlite as a data store since it combines the simplicity of normal files with some of the performance and usage benefits of databases.
Our script aims to speed up the benchmarking process by launching multiple instances of fsbench on the different cores of the system.
For this we decided to use the python process pool implementation as it provides a simple way to split up the workload.
Our multiprocessing model therefore just works by enqueuing all files that are to be benchmarked (represented by there filenames) into a queue.
Each of the worker process will benchmark the compression of this file as soon at it is done with the previous task by starting an fsbench instance.
After each finished instance of the benchmarks, meaning one run of fsbench, the main process will get the results and write them into a sqlite database.


We provide a secondary script to help visualize the results. 
The library pyplot to help with this visualization as it provides a rich interface for displaying diagrams, graphs and the like.
The visualization script aims to provide some capabilities to filter the data and get a good first impression of the gathered data and was also used to create the diagrams in this document.

Along the way we encountered a few issues, after first implementing the whole thing using pythons thread safe queue we noticed that we needed further abstraction in order to ensure a readable script. We used the python process pool to achieve this.

Fsbench in it's non interactive mode, meaning the csv mode that is used for machine parsable results, also does not supply results such as the compression ratio but instead provides the data to calculate thes yourself. We decided to store data like this in the database since operations such as averaging the compression ratio of many files would otherwise be very slow.
While views would have been an option we decided that some 20-30\% increase in result size would be more bearable than contently having slow querys while analyzing the data.

\chapter{Measuring and Processing}
\label{Measuring and Processing}
%TODO continue
\section{Data Sources}
The examined data sets come from 5 sources and contain different kinds of information.
The first source is the KASCADE Cosmic Ray Data Centre (KCDC)\footnote{\url{https://kcdc.ikp.kit.edu/}} of the Karlsruhe Institute of Technology which provides data about cosmic rays. 
The data is stored in an ASCII format. 
\\
The second source provides climatic data and is the Cera database from the DKRZ\footnote{\url{http://cera-www.dkrz.de/CERA/}}. 
It is stored as in the NetCDF format. 
\\
The third amount of data sets is provided by the National Climatic Data Center (NCDC) and their global forecast system\footnote{\url{http://www.ncdc.noaa.gov/data-access/model-data/model-datasets/global-forcast-system-gfs}}. 
It provides weather data in the grb2 format with inv metadata. 
\\
The fourth source is the CERN\footnote{\url{http://opendata.cern.ch/}} which provides data from the Large Hadron Collider (LHC) in the root format (for their data analysis software). 
\\
Our final data set is the Silesia compression Corpus\footnote{\url{http://sun.aei.polsl.pl/~sdeor/index.php?page=silesia}}, a standard data set for testing lossless compression algorithms, therefor it contains a mutitude of data and formats.
\\
\newpage

\section{Results}
The first approach was to test algorithms on the Silesia Corpus to filter some inappropriate algorithms and test the rest on all the data sets seperately and all the sets together.
\\
Testing on the Silesia Compression Corpus lead to the results in figure \ref{fig:sc_res}. 
The corresponding values are shown in table \ref{tab:sc_res}. \\


%Data from benchmark_26031700.db, filtered with: %silesia%
\begin{figure}[h]
\begin{center}
	\scalebox{0.75}{\input{plots/silesia_1.pgf}}
\end{center}
\caption{SC results plot}
\label{fig:sc_res}
\end{figure}

\quad \\

%Data from benchmark_26031700.db, filtered with: %silesia%
\begin{table}
\begin{center}
\scalebox{0.9}{
\begin{tabular}{|l|r|r|}
	\hline
	Algorithm & Compression Ratio & Compression speed MByte/s \\ \hline
	7z-deflate & 3.86 & 7.79 \\
	7z-deflate64 & 4.00 & 7.03 \\
	bzip2 & 5.32 & 8.13 \\
	LZ4 & 2.26 & 346.98 \\
	lzjb & 1.76 & 172.20 \\
	lzmat & 3.25 & 27.10 \\
	LZO & 2.25 & 563.18 \\
	zlib & 3.69 & 21.29 \\
	ZSTD & 3.42 & 171.62 \\ \hline
\end{tabular}
} 
\caption{SC results table}
\label{tab:sc_res}
\end{center}
\end{table}

\section{evaluation}
	
\chapter{Discussion}
\label{Discussion}


\chapter{Conclusion}
\label{Conclusion}
%TODO write

\nocite{*}
\bibliographystyle{alpha}
\bibliography{literatur}

%TODO : an ref s denken!
\listoffigures

\listoftables

\lstlistoflistings

%TODO : was?
\newpage

\thispagestyle{empty}

\chapter*{}

%TODO needed?
\section*{Erklärung}

Wir versichern, dass wir die Arbeit selbstständig verfasst, die Arbeiten am Projekt selbstständig durchgeführt und keine anderen, als die angegebenen Hilfsmittel -- insbesondere keine im Quellenverzeichnis nicht benannten Internetquellen -- benutzt haben, die Arbeit vorher nicht in einem anderen Prüfungsverfahren eingereicht haben.

\smallskip

\bigskip
\bigskip
\bigskip

%Hamburg, den 29.03.2015  \quad \dotfill \\ \\

\end{document}
