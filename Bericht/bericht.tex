\documentclass[
	12pt,
	a4paper,
	BCOR10mm,
	%chapterprefix,
	DIV14,
	listof=totoc,
	bibliography=totoc,
	headsepline
]{scrreprt}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{lmodern}

\usepackage[footnote]{acronym}
\usepackage[page,toc]{appendix}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{graphicx}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[htt]{hyphenat}
\usepackage{listings}
\usepackage{lscape}
\usepackage{microtype}
\usepackage{nicefrac}
\usepackage{subfig}
\usepackage{textcomp}
\usepackage[subfigure,titles]{tocloft}
\usepackage{units}
\usepackage{pgf}
\usepackage{amsmath}
\usepackage{placeins}

\lstset{
	basicstyle=\ttfamily,
	frame=single,
	numbers=left,
	language=C,
	breaklines=true,
	breakatwhitespace=true,
	postbreak=\hbox{$\hookrightarrow$ },
	showstringspaces=false,
	tabsize=4
}

\renewcommand*{\lstlistlistingname}{Listing catalog}

\renewcommand*{\appendixname}{Appendix}
\renewcommand*{\appendixtocname}{Appendices}
\renewcommand*{\appendixpagename}{Appendices}


\begin{document}

\begin{titlepage}
	\begin{center}
		{\titlefont\huge Kompression: Wissenschaftliche Daten mit unterschiedlichen Algorithmen \par}

		\bigskip
		\bigskip

		{\titlefont\Large --- Projektbericht ---\par}

		\bigskip
		\bigskip

		{\large Arbeitsbereich Wissenschaftliches Rechnen\\
		Fachbereich Informatik\\
		Fakultät für Mathematik, Informatik und Naturwissenschaften\\
		Universität Hamburg\par}
	\end{center}

	\vfill

	{\large \begin{tabular}{ll}
		Vorgelegt von: & Lukas Stabe \\
			& Hans Ole Hatzel \\
			& Arne Struck \\
		E-Mail-Adresse: 
			& \href{mailto:2stabe@informatik.uni-hamburg.de} {2stabe@informatik.uni-hamburg.de} \\			
			& \href{mailto:2hatzel@informatik.uni-hamburg.de}{2hatzel@informatik.uni-hamburg.de} \\
			& \href{mailto:1struck@informatik.uni-hamburg.de}{1struck@informatik.uni-hamburg.de} \\ 
		%Matrikelnummer: & 1234567 \\
		Studiengang: & Informatik \\
		\\
		Betreuer: & Michael Kuhn \\
			& Konstantinos Chasapis \\
		\\
		Hamburg, den 30.3.2015
	\end{tabular}\par}
\end{titlepage}

\chapter*{Abstract}
Compression has always been an important topic in computing but has recently risen in importance as the development of both storage capacity and speed is unable to keep up with the growth in computing power.
This report aims to compare some of the popular compression algorithms in both speed of decompression and compression as well as the compression ratio. We specifically look at scientific data as it is prevalent in HPC application.
Overall we see that, while a lot of scientific data already has compression built into the used file format, a lot of memory can be saved on uncompressed data.
Depending on whether speed or compression ratio are more important in a specific case, we recommend different compression algorithms.
\thispagestyle{empty}

   

\tableofcontents

\chapter{Introduction}
\label{Introduction}
%TODO ausbaubar

Since and even before the start of the days of modern computers compression is of a certain importance.

Before computers as we know them today became ubiqitous, compression could be found (among others) in commercial codes for telegraphing\cite{com}, or even in the still-in-use morse code.

The main reason to use a form of compression in these systems was to reduce the time it took to send a certain message. This reason carries on into the modern age in the form of compressed network communications; many other applications for compression have been discovered since then.

Among others, fast compression algorithms are used to transparently reduce the load and amount of data in live, in-use file systems. Slower algorithms might be utilized for significant reduction in data size, and thus disk usage, in long-term storage systems.

With modern computers in general, and especially in High Performance Computing (HPC) we see an emerging gap between the speed at which data can be computed and the speed at which it can be stored it in an appropriate way. 
This gap stems from the growing difference between computing speed and storage capacity\cite{ExaStoSy}.

The effect can be observed when looking at new generations of supercomputers: 
In the case of the DKRZ, projections see storage capacity increase only by a factor of three while computing performance is expected to grow by a factor of twenty\cite{ExaStoSy}.

Compression can not solve this problem, but it can help to mitigate the impact of the disproportionate growth of computing speed vs. storage speed, and thus reduce the significance of the gap. 

However, compression is a big field. There are many algorithms to achieve compression, each with it's own set of performance characteristics. To apply compression in a beneficial manner, one has to carefully choose an appropriate algorithm, or even multiple algorithms, for the use case at hand.

This leads us to the main purpose of this project: To find, through testing possible candidates in simulations of typical situations in HPC, the best algorithms to be used in high-speed computing and storage systems.


\chapter{Assignment and Methodology}
\label{Assignment and Methodology}
%TODO ausbauen
\section*{Assignment}
The main assignment for this project is the performance analysis for different compression algorithms in applications typical for HPC. We split this assignment into multiple sub-tasks:

\begin{enumerate}
\item Identify common scenarios in which compression can be applied in HPC
\item Simulate these scenarios using a set of compression algorithms, measuring the performance of each algorithm in compression/decompression speed and compression ratio
\item Analyze the resulting performance data to find an optimal algorithm for each scenario
\end{enumerate}

We also need to find or build appropriate tools to simulate, measure and analyze compression algorithms.


\section*{Methodology}
We approached this by building a tool that tests multiple compression algorithms with a set of files and writes the test results into a database for later analysis.

To simulate scenarios similar to those in HPC, we aquired a set of test data files from various sources:
\begin{itemize}
\item CERN Open Data
\item DKRZ
\item NCDC (the US National Climatic Data Center)
\item KIT (Karlsruhe Institute of Technology)
\end{itemize}

We also included a data set commonly used for testing and benchmarking compression algorithms, called Silesia Compression Corpus.

After selecting and getting suitable data sets and testing algorithms on them with our script we needed to analyse the results. To do so we created a metric which can be weighted, so we could change the way an algorithms results are rated depending on the scenario.

We then applied the metric using two different scenarios and found recommended algorithms for each one.

\chapter{Design and Implementation}
\label{Design}
%TODO expand

This chapter will first describe the initial design of the tools we developed. It will then lay out the major issues we had during implementing this design and changes we made to solve those issues. Finally, we will discuss some of the drawbacks of our solution.

\section{Design}

We planned to implement a script that would compress and decompress sets of data files, and gather statistics about the test runs.

To do this, our script utilizes fsbench. Fsbench is a free open source tool implemented in C++ that is able to test a set of compression algorithms against a file. It is, however, not capable of batch processing multiple files.

This is where our script comes in. We provide a simple-to-use interface to test entire directory structures and store the resulting data in an easily processable format. We decided to go with sqlite as a data store since it combines the simplicity of having just one database file with the performance and usage benefits taht are typical for relational databases.

Our script aims to speed up the benchmarking process by launching multiple instances of fsbench on the different processing cores available in the system.
For this we decided to use the python process pool implementation as it provides a simple way to split up the workload.
Our multiprocessing model therefore just works by enqueuing all files that are to be benchmarked (represented by there filenames) into a queue.
Each of the worker process will benchmark the compression of this file as soon at it is done with the previous task by starting an fsbench instance.

Each fsbench instance is provided the file and the list of compression algorithms to test (given as command line arguments to our script). The instance is also instructed to output test run results in the machine-readable csv format.

After each finished instance of the benchmarks, meaning one run of fsbench, the main process will retreive the results and write them into a sqlite database.

Additionaly, we provide a second script to help visualize and format the results. 
This uses the library matplotlib that provides a rich interface for displaying diagrams, graphs and the like.

We also implemented a function to generate \LaTeX tables to get the results into this report in an automated fashion.

The visualization script aims to provide some capabilities to filter the data and get a good first impression of the gathered results.

\section{Implementation}

Along the way we faced a few issues: After the first iteration which implemented the multiprocessing aspect using pythons thread safe queues we noticed that the script quickly became confusing to read. Using python's process pool, we introduced another level of abstraction in order to ensure a readable script.

We run fsbench in it's non interactive mode. This comes with the limitation that fsbench does not output calculated results such as the compression ratio but instead provides the raw data to calculate these yourself. We decided to compute those calculated results (mainly compression ratio and comression-/decompression-speed) in the database. This takes some calculation time when storing the data, but makes operations such as averaging the compression ratio of many files faster and simpler. We chose this trade-off since we expected to query data multiple times with different querys after having run a benchmark once.

While views would have been an option we decided that a 20-30\% increase in database size would be a smaller burden than contently waiting for slow querys while analyzing the data.

Since our script supports resuming the benchmark of a whole directory by just running the benchmark again we need to quickly access our database to check if a given file has already been benchmarked (with the same options). Our simple approach of a sqlite database without explicit primary keys and indices turned out to be too slow for this.
To speed the queries up, we introduced an sqlite index covering the filename and algorithm columns. This accelerated the query from about 7 seconds for one thousand files to about one second for resuming a twenty thousand entry benchmark.

Another issue we encountered relatively late was fsbench report misleading data. If fsbench recognizes a file cannot be compressed (meaning the compression ratio being barely above or equal to 1) it won't actually benchmark decompression but instead report a very large number for decompression speed (this might be the time required to copy the memory contents from the input buffer to the output buffer). We only noticed this since our averages were raising to absurd values. This issue was fixed by simply discarding results with a compression ratio of 1 when calculating decompression speed.

\section{Drawbacks}

One major drawback of our approach is that fsbench does the whole benchmark for one file in one piece and in memory. While this makes it easy for fsbench to provide unbiased results (no need to stop the measurement, copy new data into the memory and start the measurement again), it also means that we can not currently do benchmarks for files with a size greater than about \(\frac{1}{3}\) of the memory available to the system.

This does not influence our results in a meaningful way, since we compress the data in smaller blocks anyway (and those blocksizes have little impact on the results except if they are really small, see Figure \ref{fig:blocksizes}). To benchmark bigger files, it would currently be necessary to split them up into smaller chunks manually.

\chapter{Measuring and Processing}
\label{Measuring and Processing}
%TODO continue

\section{Benchmark Machines Specifications}
All benchmarks were run on a machine with the following specifications:

\begin{itemize}
\item 2 Intel Xeon X5560 mit je \(4 * 2.8\text{GHz}\)
\item 12GB RAMs
\end{itemize}

\section{Data Sources}
The examined data sets come from 5 sources and contain different kinds of information.
The first source is the KASCADE Cosmic Ray Data Centre (KCDC)\footnote{\url{https://kcdc.ikp.kit.edu/}} of the Karlsruhe Institute of Technology which provides data about cosmic rays. 
The data is stored as text in ASCII encoding. 
It's a table represented with space delimited numbers. Only small parts of the ASCII range are actually used for most of the file. Only at the beginning of the file there are headlines for each column, consisting of text, the rest of the files consists of mostly numbers.
Therefore the file is expected to be very compressible.
\\

The second source provides climatic data and is the Cera database from the DKRZ\footnote{\url{http://cera-www.dkrz.de/CERA/}}. 
It is stored as in the NetCDF format.
NetCDF is a binary format for storing array-like data that contains meta data describing the stored data.
%TODO what about internal compression of netcdf
\cite{NetCDF}
\\

The third set of data sets is provided by the National Climatic Data Center (NCDC) and their global forecast system\footnote{\url{http://www.ncdc.noaa.gov/data-access/model-data/model-datasets/global-forcast-system-gfs}}. 
We used data weather data provided in the grb2 format with inv files for metadata. 
\\

The fourth source is the CERN\footnote{\url{http://opendata.cern.ch/}} which provides data from the Large Hadron Collider (LHC) in the root format (for their data analysis software).
ROOT files aim to be very compressed binary files. Therefore little compression is to be expected, but many files in HPC may be precompressed, therefore performance on compressed data could also be a very interesting metric.
%TODO source here: https://root.cern.ch/drupal/content/root-files-1
\\

Our final data set is the Silesia compression Corpus\footnote{\url{http://sun.aei.polsl.pl/~sdeor/index.php?page=silesia}}, a standard data set for testing lossless compression algorithms, therefor it contains a mutitude of data and formats. We used this to gain an initial impression of the algorithms and which aspects they excel in. While this dataset aims to be as diverse as possible, some of the datasets are also typical for HPC.
\\

\newpage
\section*{Block sizes}
Compression algorithms in HPC would ideally work in parallel as otherwise resources of HPC-systems are not used to their full extend.
A common approach for achieving parallelism in compression is by splitting up the data and compressing each the blocks individually on a different core or node\cite{BenComp}. \\
While we won't explore the implementations for splitting up the data and distributing it we can access the performance of different algorithms on smaller blocks of data to simulate their usage in parallel implementations. 
By default our tools test with a block size of 1.7 GB, meaning that the implementation of each algorithm gets 1.7GB of data at a time. 
Some algorithms may split up this data on their own, meaning a smaller block size might not have any effect. \\%TODO source for lz4
To simulate realistic splitting of these files for multiprocessing we decided to go with block sizes of 500MB, 100MB, 10MB, 1MB, and 0.1MB.

\input{data_files/blocksizes.tex}
This data clearly show that the block sizes do not have a large impact on the performance of compression algorithms. 
In general a trend of less but faster compression for smaller block sizes can be observed.
This can be attributed to the fact that the algorithms have a smaller 'backlog' of data to look for possible compression and therefore operate faster\cite{ResComp}. 
Lowering the block size may actually have a positive impact on performance and not affect compression ratio.

For block sizes down to 1MB compression ratio isn't noticeably affected.
While in general performance goes up with lower block size 500MB seems to be the only out lier.
We presume that this is due to the added overhead of launching multiple iterations and due to the algorithms splitting up data internally as well, meaning the step from 1.7 GB down to 500MB might not actually affect them.
The data does however show that even for 0.1MB the impact on compression ratio is minimal (below $10\%$). 
Therefore in general our algorithms seem to be viable for parallel compression as well.

\FloatBarrier
\section{Results}
\label{Results}
The first approach is to test algorithms on the Silesia Corpus to decide which algorithms will be looked at in more detail.
\\
Testing on the Silesia Compression Corpus leads to the results in figure \ref{fig:sc_res}. 
The corresponding values are shown in table \ref{tab:sc_res}. \\



% silesia


\input{data_files/silesia.tex}
The graph looks at compression speed and compression ratio. A point to the top tight of another one means that, for the two metrics and our given dataset, the top right algorithms is flat out better. Given this dataset we decided to take a closer look at both lz4 and lzo since these are the algorithms that mostly focus on the speed and sacrifice compression ratio. We will take a look at zstd since it appears to make a uniquely balanced trade-off. \\
For the other extreme of the speed compression ratio trade-off we decided to go with bzip2. Both 7z-deflate variants are slower than bzip2 on the given dataset therefore we decided to go with zlib as well as an algorithm a little more balanced towards the speed aspect.






% all samples


\input{data_files/compression-colors.tex}
\input{data_files/compression-colors-log.tex}
%\FloatBarrier
Figure \ref{fig:much_color} and \ref{fig:much_color_log} show all of our samples. The most interesting part is the very high compression speed for files with a compression ratio of around $1.0$. This can be attributed to some of the algorithms recognizing that they operate on pre-compressed data, like the .root file from CERN Open-data.

The graph also gives a relativity simple overview of where most of the samples fall for each algorithm.
Another intresting aspect is the fact that each of the algorithms roughly describes a linear function with their different samples (excluding the points at compression ratio $1.0$ with high compression speeds).
This means that we generally have a linear relationship between compression ratio and compression speed.
\FloatBarrier





% kcdc


%TODO Text something to ncdc
\input{data_files/kcdc.tex}
This data (Figure \ref{fig:kcdc_comp}) set, while yielding no surprises in algorithm performance, shows the impact compression can actually have. With a compression ratio of 2.5 the size of a file is cut in less then half.
The data representation was however also very poorly designed as it stores mostly numbers in represented as characters.
The possible compression ratio would be a lot lower if the data was actually stored as a binary float or integer representation.





% ncdc


\FloatBarrier
\input{data_files/ncdc.tex}
%TODO Text something to kcdc
We expected the compression algorithms to at least give us some compression.
However the compression ratio even for the slower algorithms is barely above $1.0$
The GRIB2 files appear to also be compressed. GRIB2 does support file format specific compression \cite{GRIMP}.





% cern


\FloatBarrier 
\input{data_files/cern.tex}

The .root file format is already compressed an therefore none of our algorithms can achieve any meaningful compression. More interesting is the fact that some of the algorithms are able to finish very quickly with a compression ratio of 1.0. 
Meaning that these algorithms have built in detections for when a file won't compress well and can react accordingly.
\FloatBarrier




% dkrz


\FloatBarrier
\input{data_files/dkrz.tex}
The netCDF data is an interesting dataset since while lz4 and lzo seem to recognize it as incompressible other algorithms still achieve decent results.
The data format supports compression without any user interaction such as explicitly decompressing the file. It appears as though this instance of a netCDF file is compressed, as we can see in figure \ref{fig:nc_uncompressed} not all netCDF files are compressed.





% netcdf-uncompressed


\FloatBarrier
\input{data_files/netcdf-uncompressed.tex}
For netCDF files that do not have internal compression we can again achieve great compression ratios with all algorithms and get the usual split into slower and faster algorithms.





% all data


\FloatBarrier
\input{data_files/all.tex}
Joined all this results lead to the Figure \ref{fig:all_comp} with values in Table \ref{tab:all_comp}.
Furthermore you can look at the joined decompression results in Figure \ref{fig:all_decomp} with their values in Table \ref{tab:all_dec}.
\FloatBarrier

\section{Scenarios}
\label{Scenarios}

For evaluating the benchmark results and choosing algorithm recommendations, we first need to describe the different scenarios that we expect compression to be used in.

\subsection{Live File System}

The first scenario is transparent compression of the live file system used by a HPC cluster. In addition to computation results, programs write intermediate states of data and metadata to the file system and may read those to continue computation at a later point.

In this scenario, it is important that the comression and decompression speed of the algorithm chosen are fast enough as to not limit the system's I/O performance.  The algorithms should be able to compress/decompress as fast as or faster than the storage (probably HDDs) can write/read data.

Ideally, an algorithm should not slow down much on incompressible or lightly compressible data.

While the compression ratio in this scenario is not as important as the speed, a high ratio may (in addition to disk space savings) actually lead to improved I/O performance, because less data needs to be written to disk.

\subsection{Archival}

In the second scenario, data is to be compressed for archival. Since this data is not expected to be read regularly, compression/decompression speed are not as important as in scenario 1. Compression ratio is more important since it saves space required for the archive, which lowers maintenance costs.

\section{Evaluation}
In order to evaluate these results a comparison metric is required.
The resulting data can be interpreted as a set consiting of 2-tuples.
To get the values into the same value domain we have to normalize the values.
The base value for the normalization is the maximal value in its domain.
This results in (\(v(x)\) returns entry x in the tuple \(v\)):
\begin{center}
	\(
	   v(x) = \frac{v(x)}{max(v(x))}|\forall v \in \text{Results} \land \forall x \in \{\text{ratio}, \text{speed}\}
	\)
\end{center}

Since the Data consists of only positive components we can interprete the tuple as a vector and use the length of the normalized vectors as metric. 
Positive in this case connotes the greater the value the more efficient the algorithm.
At last we need to add the ability to weight the results. 
To achieve this we add a weighting vector whose elements will sum up to one and will be added as factors in the metric. 
This streches the unit circle to desired relations between compression speed and ratio.
For the normalized result vector \(V = (\text{compression ratio}, \text{speed})\) and the weighting vector \(W = (\text{compression ratio weighting}, \text{speed weighting})\) this can be expressed as:
\begin{center}
	\(
	\sum\limits^{1}_{i=0} \sqrt{(W(i) \cdot V(i))^2}
	\)
\end{center}
while
\begin{center}
	\(
	\sum\limits^{1}_{i=0}W(i) = 1
	\)
\end{center}
If we also want to adress decompression we can extend every vector with a third value for decompression speed.
Table \ref{tab:metric} shows the results of the applied metric.
There are five interesting use cases:
\begin{itemize}
	\item Compression speed and Decompression speed with higher weighting values than Compression Ratio.
	We chose this to represent live systems.  
	In live systems you have real time requirements, so every time overhead has to be minimized. 
	\\
	\item Decompression speed with a higher weighting value than compression speed and ratio.
	We chose this to represent the write once, read multiple times case, the most common one in standard computing.
	\item All three values with equal weighting.
	This is a more theoretical case to get an estimate how we value the algorithms without weighting.
	\\
	\item Compression speed with a stronger weighting than the other two.
	This shows a case where we want to write something and are not sure if we want to read it.
	\\
	\item Compression ratio with a stronger weighting value than compression speed and decompression speed.
	This is the archive case where real time components are not important as used storage.
	So we need to minimize the used storage, we can achieve this with high compression ratios.
\end{itemize}
\input{data_files/metric.tex}

\FloatBarrier
	
\chapter{Discussion}
\label{Discussion}
Overall it is clear that some algorithms will be far more usable for live file system compression while others are more viable for long term compression where a high compression ratio is most important.
There are two general use cases, one being the compression of live file systems and the other being compression for archival.
\\
Table \ref{tab:metric} suggest that lz4 is the most suitable algorithm for the live file system scenario \ref{Scenarios}.
We can derive this because live systems need good compression speed as well as good decompression speed, which stands between the cases in column 1 and 2.
Furthermore we can derive from table \ref{tab:metric} that bzip2 is the best algorithm for the archive scenario, which corresponds to the last case of the last column of table \ref{tab:metric}.
\\
In addition live file systems compression speeds should be on par with disks speeds to ensure the system can run at full speed.
lz4 and lzo both meet these requirements.
For archival one has to make the trade off of memory and computing power. 
Depending on how both factors are valued one might come to the conclusion that bzip is in fact the best algorithm. 
This supports the findings so far.
On the other hand other papers have found lz4 to have the better trade off even for long term archival\cite{PPerfHPC}.
\\
One arising questions is if our metric is a suitable one, since it does not value the trade-off zstd makes with neutral input in column 3.
But zstd is not outstanding in its measurements.
The balancing trade-off of zstd is not a perfect one, it gives more performance away in both directions.
In addition zstd has never a top measurement but it neither is one of the worst measurements in any of the covered cases.
So it is possible to conclude if we examine more than one ratio, the metric becomes even more viable, because we can get a better view of the algorithms this way.
\\
One might ask wether compression is worth for one the two scenarios from \ref{Scenarios}.
The question arises because much data might be in a compressed formate as shown in the \ref{Results}.
In Table \ref{tab:all_comp} and Figure \ref{fig:all_comp} we measure a ratio of 2 for lzo and lz4 for compressed and uncompressed data sets.
Additionally we observe a ratio of 4 for algorithm bzip, the candidate for archive purpose.
All in all if it is possible to double the storage capacity on the live system and to increase the storage capacity of the archive system by a factor of 4 you can state that compression is valid in the HPC field from the storage point of view.
\\ \\
\newpage
Compression should ideally not impact the performance of the system. 
For read write operations this is mostly the case if compression speeds keep up with storage speeds.
Our results find lz4 to have an average compression speed of 514MB/s meaning that to keep up with a storage speed of modern clusters, see table \ref{tab:storage_speeds}, one would need 800 of our machines cores. 
\begin{table}[h]
\begin{center}
\begin{tabular}{|l|r|}
    \hline
    Storage Throughput & 400GB/s \\
    \hline
    Archive Throughput &  21GB/s \\
    \hline
\end{tabular}
\caption{Storage speeds as estimated for a new Cluster in 2015 \cite{ExaStoSy}}
\label{tab:storage_speeds}
\end{center}
\end{table}
\FloatBarrier

This is by no means an impossible task as our results were measured on an older machine and a cluster has a lot of nodes at its disposal. But it means that in order to achieve a the same with bzip2 one would need around $50000$ of our cores.
%TODO what are read/write speed on clusters
\\


\chapter{Conclusion}
\label{Conclusion}
Many of the data types in HPC use compressions themselves.
This means to evaluate the usefullness of compression algorithms in the HPC field requires knowledge about the composition of the data.
Therefore the composition has to be analyzed and the used algorithms need the ability to detect already compressed files and handle this issue.
\\
Bzip2 and zlib lack this ability and therefore can only be considered appropriate if time is a very unimportant component of the system where we want to compress.
\\
If time is a valuable ressource you can derive from the results in table \ref{tab:metric} that lzo and lz4 are appropriate for the compression task. 
But if decompression speed is also considered, lz4 is the recommended algorithm.


\nocite{*}
\bibliographystyle{plain}
\bibliography{literatur}

\listoffigures

\listoftables

\newpage

\thispagestyle{empty}

\chapter*{}

%TODO needed?
\section*{Erklärung}

Wir versichern, dass wir die Arbeit selbstständig verfasst, die Arbeiten am Projekt selbstständig durchgeführt und keine anderen, als die angegebenen Hilfsmittel -- insbesondere keine im Quellenverzeichnis nicht benannten Internetquellen -- benutzt haben, die Arbeit vorher nicht in einem anderen Prüfungsverfahren eingereicht haben.

\smallskip

%Hamburg, den 29.03.2015  \quad \dotfill \\ \\

\bigskip
\bigskip
\bigskip
\end{document}
