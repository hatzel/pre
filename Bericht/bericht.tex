\documentclass[
	12pt,
	a4paper,
	BCOR10mm,
	%chapterprefix,
	DIV14,
	listof=totoc,
	bibliography=totoc,
	headsepline
]{scrreprt}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{lmodern}

\usepackage[footnote]{acronym}
\usepackage[page,toc]{appendix}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{graphicx}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage[htt]{hyphenat}
\usepackage{listings}
\usepackage{lscape}
\usepackage{microtype}
\usepackage{nicefrac}
\usepackage{subfig}
\usepackage{textcomp}
\usepackage[subfigure,titles]{tocloft}
\usepackage{units}
\usepackage{pgf}
\usepackage{amsmath}
\usepackage{placeins}

\lstset{
	basicstyle=\ttfamily,
	frame=single,
	numbers=left,
	language=C,
	breaklines=true,
	breakatwhitespace=true,
	postbreak=\hbox{$\hookrightarrow$ },
	showstringspaces=false,
	tabsize=4
}

\renewcommand*{\lstlistlistingname}{Listing catalog}

\renewcommand*{\appendixname}{Appendix}
\renewcommand*{\appendixtocname}{Appendices}
\renewcommand*{\appendixpagename}{Appendices}

\newcommand*{\mcite}[1]{\footnote{\cite{#1}}}


\begin{document}

\begin{titlepage}
	\begin{center}
		{\titlefont\huge Titel der Arbeit\par}

		\bigskip
		\bigskip

		{\titlefont\Large --- Projektbericht ---\par}

		\bigskip
		\bigskip

		{\large Arbeitsbereich Wissenschaftliches Rechnen\\
		Fachbereich Informatik\\
		Fakultät für Mathematik, Informatik und Naturwissenschaften\\
		Universität Hamburg\par}
	\end{center}

	\vfill

	{\large \begin{tabular}{ll}
		Vorgelegt von: & Lukas Stabe \\
			& Hans Ole Hatzel \\
			& Arne Struck \\
		E-Mail-Adresse: 
			& \href{mailto:2stabe@informatik.uni-hamburg.de} {2stabe@informatik.uni-hamburg.de} \\			
			& \href{mailto:2hatzel@informatik.uni-hamburg.de}{2hatzel@informatik.uni-hamburg.de} \\
			& \href{mailto:1struck@informatik.uni-hamburg.de}{1struck@informatik.uni-hamburg.de} \\ 
		%Matrikelnummer: & 1234567 \\
		Studiengang: & Informatik \\
		\\
		Betreuer: & Michael Kuhn \\
			& Konstantinos Chasapis \\
		\\
		Hamburg, den 30.3.2015
	\end{tabular}\par}
\end{titlepage}

\chapter*{Abstract}
%TODO write

\thispagestyle{empty}

   

\tableofcontents

\chapter{Introduction}
\label{Introduction}
%TODO ausbaubar

Since and even before the start of the days of modern computers compression is of a certain importance.

Before computers as we know them today became ubiqitous, compression could be found (among others) in commercial codes for telegraphing, or even in the still-in-use morse code.

The main reason to use a form of compression in these systems was to reduce the time it took to send a certain message. This reason carries on into the modern age in the form of compressed network communications; many other applications for compression have been discovered since then.

Among others, fast compression algorithms are used to transparently reduce the load and amount of data in live, in-use file systems. Slower algorithms might be utilized for significant reduction in data size, and thus disk usage, in long-term storage systems.

With modern computers in general, and especially in High Performance Computing (HPC) we see an emerging gap between the speed at which data can be computed and the speed at which it can be stored it in an appropriate way. 
This gap stems from the growing difference between computing speed and storage capacity\mcite{ExaStoSy}.

The effect can be observed when looking at new generations of supercomputers: 
In the case of the DKRZ, projections see storage capacity increase only by a factor of three while computing performance is expected to grow by a factor of twenty\mcite{ExaStoSy}.

Compression can not solve this problem, but it can help to mitigate the impact of the disproportionate growth of computing speed vs. storage speed, and thus reduce the significance of the gap. 

However, compression is a big field. There are many algorithms to achieve compression, each with it's own set of performance characteristics. To apply compression in a beneficial manner, one has to carefully choose an appropriate algorithm, or even multiple algorithms, for the use case at hand.

This leads us to the main purpose of this project: To find, through testing possible candidates in simulations of typical situations in HPC, the best algorithms to be used in high-speed computing and storage systems.


\chapter{Assignment and Methodology}
\label{Assignment and Methodology}
%TODO ausbauen
\section*{Assignment}
The main assignment for this project is the performance analysis for different compression algorithms in applications typical for HPC. We split this assignment into multiple sub-tasks:

\begin{enumerate}
\item Identify common scenarios in which compression can be applied in HPC
\item Simulate these scenarios using a set of compression algorithms, measuring the performance of each algorithm in compression/decompression speed and compression ratio
\item Analyze the resulting performance data to find an optimal algorithm for each scenario
\end{enumerate}

We also need to find or build appropriate tools to simulate, measure and analyze compression algorithms.


\section*{Methodology}
We approached this through writing a script around a tool for testing multiple compression algorithms so we could use it for multiple files and store the results in a database. 
\\
Multiple files were necessary to get an appropriate amount of measurements.
For this purpose different datasets from scientific sources were required.
\\
After selecting and getting suitable data sets and testing algorithms on them with our script we needed to analyse the different outputs.
To do so we created a metric which can be weighted, so it is possible to map the results to our use cases in an appropriate way. \\

\chapter{Design and Implementation}
\label{Design}
%TODO expand
We implemented a script to test compression algorithms on multiple datasets based on the open source tool fsbench. 
Fsbench is written in C++ and has the option to output the data it gathers, machine readable, as csv.
However the tool is not capable of batch processing multiple files. \\
This is where our script comes in, we intend to provide a simple interface to test entire directory structures and store them in easy to use formats.
We decided to go with sqlite as a data store since it combines the simplicity of normal files with some of the performance and usage benefits of databases. \\
Our script aims to speed up the benchmarking process by launching multiple instances of fsbench on the different cores of the system.
For this we decided to use the python process pool implementation as it provides a simple way to split up the workload.
Our multiprocessing model therefore just works by enqueuing all files that are to be benchmarked (represented by there filenames) into a queue.
Each of the worker process will benchmark the compression of this file as soon at it is done with the previous task by starting an fsbench instance.
After each finished instance of the benchmarks, meaning one run of fsbench, the main process will get the results and write them into a sqlite database.

We provide a secondary script to help visualize the results. 
The library matplotlib to help with this visualization as it provides a rich interface for displaying diagrams, graphs and the like.
The visualization script aims to provide some capabilities to filter the data and get a good first impression of the gathered data and was also used to create the diagrams and tables in this document.


Along the way we encountered a few issues, after first implementing the whole thing using pythons thread safe queue we noticed that we needed further abstraction in order to ensure a readable script. We used the python process pool to achieve this.

Fsbench in it's non interactive mode, meaning the csv mode that is used for machine parsable results, also does not supply results such as the compression ratio but instead provides the data to calculate these yourself. We decided to store data with the calculated results the database since operations such as averaging the compression ratio of many files would otherwise be very slow.
While views would have been an option we decided that some 20-30\% increase in result size would be more bearable than contently having slow querys while analyzing the data.

Since our script supports resuming the benchmark of a whole directory by just running the benchmark again we need to quickly access our database to check if a given file has already been benchmarked (with the same options). Our naive approach of a sqlite database without explicit primary keys and indexes turned out to be too slow for this.
We had to introduce sqlite indexes to the filename and algorithm columns to make this lookup faster. This made us go from around 7 seconds for one thousand files to around one second for the resuming a twenty thousand entry benchmark. 

Another issue we encountered relatively late was febench report misleading data. If fsbench recognizes a file cannot be compressed (meaning the compression ratio being barely above or equal to 1) it won't actually benchmark decompression but instead report a very large number for decompression speed. We only noticed this since our averages were raising to absurd values.
We ended up fixing this by just ignoring files with a compression ratio of 1 for decompression speed.

\chapter{Measuring and Processing}
\label{Measuring and Processing}
%TODO continue
\section{Data Sources}
The examined data sets come from 5 sources and contain different kinds of information.
The first source is the KASCADE Cosmic Ray Data Centre (KCDC)\footnote{\url{https://kcdc.ikp.kit.edu/}} of the Karlsruhe Institute of Technology which provides data about cosmic rays. 
The data is stored as text in ASCII encoding. 
It's a table represented with space delimited numbers. Only small parts of the ASCII range are actually used for most of the file. Only at the beginning of the file there are headlines for each column, consisting of text, the rest of the files consists of mostly numbers.
Therefore the file is expected to be very compressible.
\\

The second source provides climatic data and is the Cera database from the DKRZ\footnote{\url{http://cera-www.dkrz.de/CERA/}}. 
It is stored as in the NetCDF format.
NetCDF is a binary format for storing array-like data that contains meta data describing the stored data.
%TODO what about internal compression of netcdf
\cite{NetCDF}
%TODO source: https://www.unidata.ucar.edu/software/netcdf/docs/netcdf.html#Introduction
\\

The third set of data sets is provided by the National Climatic Data Center (NCDC) and their global forecast system\footnote{\url{http://www.ncdc.noaa.gov/data-access/model-data/model-datasets/global-forcast-system-gfs}}. 
We used data weather data provided in the grb2 format with inv files for metadata. 
\\

The fourth source is the CERN\footnote{\url{http://opendata.cern.ch/}} which provides data from the Large Hadron Collider (LHC) in the root format (for their data analysis software).
ROOT files aim to be very compressed binary files. Therefore little compression is to be expected, but many files in HPC may be precompressed, therefore performance on compressed data could also be a very interesting metric.
%TODO source here: https://root.cern.ch/drupal/content/root-files-1
\\

Our final data set is the Silesia compression Corpus\footnote{\url{http://sun.aei.polsl.pl/~sdeor/index.php?page=silesia}}, a standard data set for testing lossless compression algorithms, therefor it contains a mutitude of data and formats. We used this to gain an initial impression of the algorithms and which aspects they excel in. While this dataset aims to be as diverse as possible, some of the datasets are also typical for HPC.
\\

\newpage
\section*{Block sizes}
Compression algorithms in HPC would ideally work in parallel as otherwise resources of HPC-systems are not used to their full extend.
A common approach for achieving parallelism in compression is by splitting up the data and compressing each the blocks individually on a different core or node\mcite{BenComp}. \\
While we won't explore the implementations for splitting up the data and distributing it we can access the performance of different algorithms on smaller blocks of data to simulate their usage in parallel implementations. 
By default our tools test with a block size of 1.7 GB, meaning that the implementation of each algorithm gets 1.7GB of data at a time. 
Some algorithms may split up this data on their own, meaning a smaller block size might not have any effect. \\%TODO source for lz4
To simulate realistic splitting of these files for multiprocessing we decided to go with block sizes of 500MB, 100MB, 10MB, 1MB, and 0.1MB.

\input{data_files/blocksizes.tex}
This data clearly show that the block sizes do not have a large impact on the performance of compression algorithms. 
In general a trend of less but faster compression for smaller block sizes can be observed.
This can be attributed to the fact that the algorithms have a smaller 'backlog' of data to look for possible compression and therefore operate faster.
Lowering the block size may actually have a positive impact on performance and not affect compression ratio.

For block sizes down to 1MB compression ratio isn't noticeably affected.
While in general performance goes up with lower block size 500MB seems to be the only out lier.
We presume that this is due to the added overhead of launching multiple iterations and due to the algorithms splitting up data internally as well, meaning the step from 1.7 GB down to 500MB might not actually affect them.
The data does however show that even for 0.1MB the impact on compression ratio is minimal (below $10\%$). 
Therefore in general our algorithms seem to be viable for parallel compression as well.

\FloatBarrier
\section{Results}
The first approach is to test algorithms on the Silesia Corpus to decide which algorithms will be looked at in more detail.
\\
Testing on the Silesia Compression Corpus leads to the results in figure \ref{fig:sc_res}. 
The corresponding values are shown in table \ref{tab:sc_res}. \\

\input{data_files/silesia.tex}
The graph looks at compression speed and compression ratio. A point to the top tight of another one means that, for the two metrics and our given dataset, the top right algorithms is flat out better. Given this dataset we decided to take a closer look at both lz4 and lzo since these are the algorithms that mostly focus on the speed and sacrifice compression ratio. We will take a look at zstd since it appears to make a uniquely balanced trade-off. \\
For the other extreme of the speed compression ratio trade-off we decided to go with bzip2. Both 7z-deflate variants are slower than bzip2 on the given dataset therefore we decided to go with zlib as well as an algorithm a little more balanced towards the speed aspect.

\input{data_files/compression-colors.tex}
Figure \ref{fig:much_color} shows all of our samples. The most interesting part is the very high compression speed for files with a compression ratio of around $1.0$. This can be attributed to some of the algorithms recognizing that they operate on pre-compressed data, like the .root file from CERN Open-data.

The graph also gives a relativity simple overview of where most of the samples fall for each algorithm.
Another intresting aspect is the fact that each of the algorithms roughly describes a linear function with their different samples (excluding the points at compression ratio $1.0$ with high compression speeds).
This means that we generally have a linear relationship between compression ratio and compression speed.
\FloatBarrier

\input{data_files/ncdc.tex}
%TODO Text something to ncdc

\input{data_files/kcdc.tex}
%TODO Text something to kcdc

\input{data_files/cern.tex}
%TODO Text something to cern

\input{data_files/dkrz.tex}
%TODO Text something to dkrz
\FloatBarrier

\input{data_files/all.tex}
This summarizes to the Figure \ref{fig:all_comp} (values in Table \ref{tab:all_comp})

\FloatBarrier

\section{Evaluation}
In order to evaluate these results a comparison metric is required.
The resulting data can be interpreted as a set consiting of 2-tuples.
To get the values into the same value domain we have to normalize the values.
The base value for the normalization is the maximal value in its domain.
This results in (\(v(x)\) returns entry x in the tuple \(v\)):
\begin{center}
	\(
	   v(x) = \frac{v(x)}{max(v(x))}|\forall v \in \text{Results} \land \forall x \in \{\text{ratio}, \text{speed}\}
	\)
\end{center}

Since the Data consists of only positive components we can interprete the tuple as a vector and use the length of the normalized vectors as metric. 
Positive in this case connotes the greater the value the more efficient the algorithm.
At last we need to add the ability to weight the results. 
To achieve this we add a weighting vector whose elements will sum up to one and will be added as factors in the metric. 
This streches the unit circle to desired relations between compression speed and ratio.
For the normalized result vector \(V = (\text{compression ratio}, \text{speed})\) and the weighting vector \(W = (\text{compression ratio weighting}, \text{speed weighting})\) this can be expressed as:
\begin{center}
	\(
	\sum\limits^{1}_{i=0} \sqrt{(W(i) \cdot V(i))^2}
	\)
\end{center}
while
\begin{center}
	\(
	\sum\limits^{1}_{i=0}W(i) = 1
	\)
\end{center}
If we also want to adress decompression we can extend every vector with a third value for decompression speed.
Table \ref{tab:metric} shows the results of the applied metric.
There are five interesting use cases:
\begin{itemize}
	\item Compression speed and Decompression speed with higher weighting values than Compression Ratio.
	We chose this to represent live systems.  
	In live systems you have real time requirements, so every time overhead has to be minimized. 
	\\
	\item Decompression speed with a higher weighting value than compression speed and ratio.
	We chose this to represent the write once, read multiple times case, the most common one in standard computing.
	\item All three values with equal weighting.
	This is a more theoretical case to get an estimate how we value the algorithms without weighting.
	\\
	\item Compression speed with a stronger weighting than the other two.
	This shows a case where we want to write something and are not sure if we want to read it.
	\\
	\item Compression ratio with a stronger weighting value than compression speed and decompression speed.
	This is the archive case where real time components are not important as used storage.
	So we need to minimize the used storage, we can achieve this with high compression ratios.
\end{itemize}
\input{data_files/metric.tex}
\FloatBarrier
	
\chapter{Discussion}
\label{Discussion}
Overall it is clear that some algorithms will be far more usable for live file system compression while others are more viable for long term compression where a high compression ratio is most important.
There are two general use cases, one being the compression of live file systems and the other being compression for archival.
For live file systems compression speeds should be on par with disks speeds to ensure the system can run at full speed.
lz4 and lzo both meet these requirements.
For archival one has to make the trade off of memory and computing power. Depending on how both factors are valued one might come to the conclusion that bzip is in fact the best algorithm. But other papers have found lz4 to have the better trade off even for long term archival. %TODO Source.

\begin{table}
\begin{center}
\begin{tabular}{|l|r|}
    \hline
    Storage Throughput & 400GB/s \\
    \hline
    Archive Throughput &  21GB/s \\
    \hline
\end{tabular}
\caption{Storage speeds as estimated for a new Cluster in 2015 \cite{ExaStoSy}}
\label{tab:storage_speeds}
\end{center}
\end{table}

Compression should ideally not impact the performance of the system. 
For read write operations this is mostly the case if compression speeds keep up with storage speeds.
Our results find lz4 to have an average compression speed of 514MB/s meaning that to keep up with a storage speed of modern clusters, see table \ref{tab:storage_speeds}, one would need 800 of our machines cores. 

This is by no means an impossible task as our results were measured on an older machine and a cluster has a lot of nodes at its disposal. But it means that in order to achieve a the same with bzip2 one would need around $50000$ of our cores.
%TODO what are read/write speed on clusters

\chapter{Conclusion}
\label{Conclusion}
Many of the data types in HPC use compressions themselves.
This means to evaluate the usefullness of compression algorithms in the HPC field requires knowledge about the composition of the data.
Therefore the composition has to be analyzed.
\\




\nocite{*}
\bibliographystyle{alpha}
\bibliography{literatur}

\listoffigures

\listoftables

\newpage

\thispagestyle{empty}

\chapter*{}

%TODO needed?
\section*{Erklärung}

Wir versichern, dass wir die Arbeit selbstständig verfasst, die Arbeiten am Projekt selbstständig durchgeführt und keine anderen, als die angegebenen Hilfsmittel -- insbesondere keine im Quellenverzeichnis nicht benannten Internetquellen -- benutzt haben, die Arbeit vorher nicht in einem anderen Prüfungsverfahren eingereicht haben.

\smallskip

%Hamburg, den 29.03.2015  \quad \dotfill \\ \\

\bigskip
\bigskip
\bigskip
\end{document}
